---
# Main playbook to deploy the component
# The different plays just serve as logical sepparations between deployment phases. If the "hosts" parameter of different plays is the same, feel free to join them
# This sample file will try to use as many public task files as possible, but your component may not need many of them. Feel free to use just the ones you need.
- name: "STAGE 1: Apply IAC to deploy the component"
  hosts: localhost
  gather_facts: false
  connection: local
  tasks:

    # Necessary in most if not all components. Loads the usable variables from 4 different sources in the following order:
    # 1. Site variables. Metadata unique to each site. Information like Image IDs, VNet IDs or ssh public keys.
    # 2. Private variables. Loads the component private values for the site hypervisor. File at {{ component_type }}/variables/{{ site_hypervisor }}/private.yaml
    # 3. Public variables. Loads the component input values as defined in the TNLCM/Jenkins. File is written and then read from {{ component_type }}/variables/input_file.yaml
    # 4. Pipeline parameters. Some of the parameters sent by the TNLCM to the Jenkins, like the custom_name or the the tnlcm_calback URL.
    #    File is written and then read from {{ component_type }}/variables/pipeline_parameters.yaml
    - name: Load enviromental variables from different sources
      ansible.builtin.include_tasks: "{{ workspace }}/.global/cac/load_variables.yaml"

    # Some components lik the tn_init can only be deployed once per TN. entity_name is a variable combining the component_type and the custom_name
    # You can optionally supress the custom_name for these components
    # - name: Overwrite "entity_name" with only tn_init
    #   ansible.builtin.set_fact:
    #     entity_name: "tn_init"

    # Creates a terraform working directory at .terraform/, loads the MINIO backend and { site_hypervisor }} terraform provider files,
    # downloads manifests from previous deployments, and runs a single terraform init
    - name: Prepare terraform working directory
      ansible.builtin.include_tasks: "{{ workspace }}/.global/cac/terraform_workdir.yaml"

    # More than just a simple terraform apply. Templates all manifests from {{ component_type }}/code/{{ site_hypervisor }}/iac/*.tf.j2, writes them
    # at .terraform and runs a terraform apply.
    # Task also has error handling, and if apply fails, will send a notification to the TNLCM with the stderr, and text from {{ component_type }}/result_templates/fail_result.md.j2
    - name: Terraform apply
      ansible.builtin.include_tasks: "{{ workspace }}/.global/cac/terraform_apply.yaml"


# Post-terraform preliminary tasks. Still no component configuration.
- name: "STAGE 2: Prepare to access the component"
  hosts: localhost
  gather_facts: false
  connection: local
  tasks:

    # Module to retrieve all generated outputs from the terraform working directory in JSON with a key-value format
    - name: Retrieve terraform outputs
      ansible.builtin.shell:
      args:
        chdir: "{{ workspace }}/.terraform/"
        cmd: "set -o pipefail && terraform output --json | jq 'with_entries(.value |= .value)'"
        executable: /bin/bash
      register: terraform_outputs
      changed_when: false

    # Save some of the outputs as playbook facts (variables). Some of them will be surelly be used as future output values
    - name: Set Terraform outputs as playbook facts
      ansible.builtin.set_fact:
        bastion_ip: "{{ (terraform_outputs.stdout | from_json)['tn_bastion-ips'][site_networks_id.default | string] }}"
        tn_ssh_public_key: "{{ (terraform_outputs.stdout | from_json)['tn_ssh_public_key'] }}"
        ips: "{{ (terraform_outputs.stdout | from_json)[entity_name + '-ips'] }}"
        id: "{{ (terraform_outputs.stdout | from_json)[entity_name + '-id'] }}"
        vnet_id: "{{ (terraform_outputs.stdout | from_json)[one_ks8500runner_networks[0] + '-id'] | string}}"

        #ks8500runner_backend_url: "{{ (terraform_outputs.stdout | from_json)[entity_name + '-ks8500runner_backend_url'] | string}}"
        #ks8500runner_registration_token: "{{ (terraform_outputs.stdout | from_json)[entity_name + '-ks8500runner_registration_token'] | string}}"
        #ks8500runner_name: "{{ (terraform_outputs.stdout | from_json)[entity_name + '-ks8500runner_name'] | string}}"

    # Include the deployed component/s as Inventory host/s.
    - name: Add new VM to Ansible Inventory
      ansible.builtin.add_host:
        hostname: "{{ entity_name }}"
        ansible_host: "{{ ips[ vnet_id ] }}"
        ansible_ssh_common_args: "-J jenkins@{{ bastion_ip }}"
        ansible_user: "jenkins"

    - name: Create SSH config file in the Jenkins-master for debugging purposes
      ansible.builtin.include_tasks: "{{ workspace }}/{{ component_type }}/code/{{ site_hypervisor }}/cac/01_pre/ssh_config.yaml"

# Configuration play. Modules executed in the deployment component.
# Remember in Ansible variables are loaded in each host, so if you change the host, you can either
# - Set the variables again (with the load_variables.yaml global taskfile or similar)
# - Refer to the hostvars of localhost.
- name: "STAGE 3: Apply CAC to prepare the component"
  vars:
    ks8500runner_mounts:
      - Config
      - SessionLogs
      - Settings
      - Storage
    ks8500runner_path: /opt/KS8500Runner
  hosts: "{{ hostvars['localhost']['entity_name'] }}"
  gather_facts: false
  tasks:
    - name: Wait for system to become reachable
      ansible.builtin.wait_for_connection:
        connect_timeout: 5
        timeout: 200

    # Some sites may have a MASTER ssh key, appart of the jenkins one
    - name: Set site ssh key as authorized in jenkins user
      ansible.posix.authorized_key:
        user: jenkins
        state: present
        key: "{{ item }}"
      loop:
        - "{{ hostvars['localhost'].get('site_admin_ssh_public_key', '') }}"
      when: item != ''

    # Detect if required network connectivity is availabla for the runner
    # Curl returns error if we cannot connect, should cause ansible script to terminate
    - name: verify-connectivity
      ansible.builtin.shell:  curl https://test-automation.pw.keysight.com/index.html
    
    - name: install-docker
      community.general.apk:
        name: docker,docker-cli-buildx,docker-compose,openrc,docker-cli-compose
        update_cache: true
    
    - name: enable-docker-service
      ansible.builtin.service:
        name: docker
        state: started
        enabled: yes

    - name: create-runner-dirs
      ansible.builtin.file:
        path: "{{ ks8500runner_path }}/{{ item }}"
        state: directory
        mode: '0700'
      loop: "{{ ks8500runner_mounts }}"

    - name: deploy-runner-configuration
      ansible.builtin.copy:
        src:  "{{ playbook_dir }}/all/runner/"
        dest: "{{ ks8500runner_path }}"
       
    - name: build-runner-image
      community.docker.docker_image_build:
        name: ks8500_runner:latest
        path: "{{ ks8500runner_path }}"
        dockerfile: Dockerfile
  
    - name: create-mount-list
      set_fact:
        mount_list: '{{ (mount_list|default([])) + [{"target": "/opt/tap/" + item, "source": "{{ ks8500runner_path }}/" + item, "type": "bind" }] }}'
      loop: "{{ ks8500runner_mounts }}"

    # The microsoft base image uses this ID. Maybe we can find a better solution.
    - name: workaround-permissions
        ansible.builtin.shell: chown -R 1654:1654 /opt/KS8500Runner

    - name: register-runner
      community.docker.docker_container:
        image: ks8500_runner
        name: register-runner
        mounts: "{{ mount_list }}"
        entrypoint: ""
        detach: false
        working_dir: "{{ ks8500runner_path }}"
        command: /opt/tap/tap runner register --name {{ ks8500runner_name }} --url {{ ks8500runner_backend_url }} --registrationToken {{ ks8500runner_registration_token }}
      register: docker_container_output

    - name: start-runner
      # cannot use docker_compose_v2 with the default package from apk, so using the deprecated docker_compose
      community.docker.docker_compose:
        project_src: "{{ ks8500runner_path }}"
# Final phase of the deployments. Gather information and publish it
- name: "STAGE 4: Publish execution results"
  hosts: localhost
  gather_facts: false
  connection: local
  tasks:
    # Common task to create custom terraform outputs with information of interest for future deployments.
    - name: Merge all component component outputs into one "output" variable
      ansible.builtin.set_fact:

        output:
          ips: "{{ ips | b64encode }}"
          id: "{{ id | b64encode }}"
          registration_info: "{{ (docker_container_output.container.Output | b64encode) }}" 
